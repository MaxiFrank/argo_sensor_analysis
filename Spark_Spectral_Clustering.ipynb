{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib import linalg\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.csv(\"./digit-recognizer/train.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(arr):\n",
    "    return float(arr[0].dot(arr[1])/\n",
    "                 ((arr[0].dot(arr[0])**0.5) * (arr[1].dot(arr[1])**0.5)))\n",
    "\n",
    "def laplacian_vector(row_id,arr,size,k):\n",
    "    lap_vec = np.zeros(size,dtype=int)\n",
    "    lap_vec[np.array(arr)] = 1\n",
    "    lap_vec[row_id] = -k\n",
    "    return list([int(item) for item in lap_vec])\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, t.DoubleType())\n",
    "laplacian_vector_udf = udf(laplacian_vector, t.ArrayType(t.IntegerType()))\n",
    "\n",
    "#sqlContext.registerFunction('COS_SIM',cosine_similarity,returnType=t.DoubleType())\n",
    "#sqlContext.registerFunction(\"LAP_VECTOR\",laplacian_vector,returnType=t.ArrayType(t.IntegerType()))\n",
    "\n",
    "class Spectral_Clustering():\n",
    "    def __init__(self,k=2,k_nearest=7,num_eigenvectors = 10,featureCol='features',predictionCol='predictions'):\n",
    "        self.k = k\n",
    "        self.k_nearest = k_nearest\n",
    "        self.num_eigenvectors = num_eigenvectors\n",
    "        self.featureCol = featureCol\n",
    "        self.predictionCol = predictionCol\n",
    "    def cluster(self, df):\n",
    "        #sqlContext = SQLContext(SparkContext.getOrCreate())\n",
    "        n = df.count()\n",
    "        # index rows\n",
    "        df_index = df.select((row_number().over(Window.partitionBy().orderBy(self.featureCol)) - 1).alias('id'),\"*\")\n",
    "        df_features = df_index.select('id',self.featureCol)\n",
    "        \n",
    "        \n",
    "        # prep for joining\n",
    "        left_df = df_features.select(df_features['id'].alias('left_id'),\n",
    "                                     df_features[self.featureCol].alias('left_features'))\n",
    "        right_df = df_features.select(df_features['id'].alias('right_id'),\n",
    "                                      df_features[self.featureCol].alias('right_features'))\n",
    "        # join on self where left_id does not equal right_id\n",
    "        joined_df = left_df.join(right_df,left_df['left_id'] != right_df['right_id'])\n",
    "        \n",
    "        # comupte cosine similarity between vectors\n",
    "        joined_df = joined_df.select('left_id','right_id',\n",
    "                                     cosine_similarity_udf(array(joined_df['left_features'],\n",
    "                                                                 joined_df['right_features'])).alias('norm'))\n",
    "        ranked = joined_df.select('left_id','right_id',rank().over(Window.partitionBy('left_id').orderBy('norm')).alias('rank'))\n",
    "        knn = ranked.where(ranked['rank'] <= 5)\n",
    "        knn_grouped = knn.groupBy('left_id').agg(f.collect_list('right_id').alias('nn'))\n",
    "        laplacian = knn_grouped.select('left_id', laplacian_vector_udf(knn_grouped['left_id'], knn_grouped['nn'], \n",
    "                                                                       lit(n), lit(self.k_nearest)).alias('lap_vector'))\n",
    "\n",
    "        laplacian_matrix = RowMatrix(laplacian.select('lap_vector').rdd.map(lambda x:x[0]))\n",
    "        eigenvectors = laplacian_matrix.computePrincipalComponents(k=self.num_eigenvectors)\n",
    "        \n",
    "        eigenvectors = [(idx,Vectors.dense([float(item) for item in row])) \n",
    "                        for idx, row in enumerate(eigenvectors.toArray().tolist())]\n",
    "        \n",
    "        eigen_df = spark.createDataFrame(eigenvectors,['id',self.featureCol])\n",
    "        model = KMeans(featuresCol=self.featureCol,predictionCol=self.predictionCol,k=self.k).fit(eigen_df)\n",
    "        predictions = model.transform(eigen_df).join(df_index,on='id')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n",
      "| id|predictions|label|\n",
      "+---+-----------+-----+\n",
      "|  0|          0|    6|\n",
      "|  1|          0|    1|\n",
      "|  2|          0|    6|\n",
      "|  3|          0|    6|\n",
      "|  4|          0|    6|\n",
      "|  5|          0|    6|\n",
      "|  6|          0|    6|\n",
      "|  7|          0|    6|\n",
      "|  8|          0|    2|\n",
      "|  9|          0|    6|\n",
      "| 10|          0|    6|\n",
      "| 11|          0|    3|\n",
      "| 12|          1|    2|\n",
      "| 13|          0|    5|\n",
      "| 14|          0|    5|\n",
      "| 15|          1|    0|\n",
      "| 16|          0|    5|\n",
      "| 17|          0|    0|\n",
      "| 18|          0|    3|\n",
      "| 19|          1|    3|\n",
      "| 20|          0|    2|\n",
      "| 21|          0|    0|\n",
      "| 22|          0|    2|\n",
      "| 23|          1|    0|\n",
      "| 24|          1|    1|\n",
      "| 25|          0|    1|\n",
      "| 26|          1|    0|\n",
      "| 27|          1|    1|\n",
      "| 28|          0|    3|\n",
      "| 29|          0|    0|\n",
      "| 30|          0|    1|\n",
      "| 31|          1|    1|\n",
      "| 32|          0|    1|\n",
      "| 33|          0|    1|\n",
      "| 34|          0|    0|\n",
      "| 35|          1|    1|\n",
      "| 36|          0|    4|\n",
      "| 37|          1|    1|\n",
      "| 38|          1|    3|\n",
      "| 39|          0|    3|\n",
      "| 40|          0|    2|\n",
      "| 41|          0|    4|\n",
      "| 42|          0|    3|\n",
      "| 43|          0|    2|\n",
      "| 44|          1|    4|\n",
      "| 45|          0|    8|\n",
      "| 46|          0|    4|\n",
      "| 47|          0|    8|\n",
      "| 48|          1|    4|\n",
      "| 49|          1|    4|\n",
      "| 50|          0|    8|\n",
      "| 51|          1|    3|\n",
      "| 52|          0|    3|\n",
      "| 53|          1|    3|\n",
      "| 54|          1|    8|\n",
      "| 55|          0|    9|\n",
      "| 56|          0|    5|\n",
      "| 57|          0|    1|\n",
      "| 58|          1|    1|\n",
      "| 59|          0|    1|\n",
      "| 60|          1|    1|\n",
      "| 61|          1|    8|\n",
      "| 62|          1|    1|\n",
      "| 63|          0|    4|\n",
      "| 64|          1|    4|\n",
      "| 65|          1|    4|\n",
      "| 66|          1|    1|\n",
      "| 67|          0|    2|\n",
      "| 68|          1|    2|\n",
      "| 69|          1|    3|\n",
      "| 70|          0|    9|\n",
      "| 71|          0|    2|\n",
      "| 72|          0|    7|\n",
      "| 73|          1|    4|\n",
      "| 74|          0|    9|\n",
      "| 75|          0|    4|\n",
      "| 76|          0|    9|\n",
      "| 77|          0|    9|\n",
      "| 78|          0|    8|\n",
      "| 79|          1|    5|\n",
      "| 80|          1|    9|\n",
      "| 81|          0|    9|\n",
      "| 82|          0|    9|\n",
      "| 83|          0|    9|\n",
      "| 84|          0|    0|\n",
      "| 85|          0|    9|\n",
      "| 86|          1|    0|\n",
      "| 87|          0|    7|\n",
      "| 88|          1|    7|\n",
      "| 89|          1|    7|\n",
      "| 90|          0|    2|\n",
      "| 91|          0|    7|\n",
      "| 92|          1|    2|\n",
      "| 93|          0|    7|\n",
      "| 94|          0|    9|\n",
      "| 95|          1|    9|\n",
      "| 96|          0|    9|\n",
      "| 97|          1|    7|\n",
      "| 98|          0|    9|\n",
      "| 99|          1|    5|\n",
      "+---+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spec_clust = Spectral_Clustering()\n",
    "assembler = VectorAssembler(outputCol='features',inputCols=df.columns[1:])\n",
    "df_features = assembler.transform(df).limit(100)\n",
    "spec_clust.cluster(df_features).select('id','predictions','label').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('distributedcomputing': conda)",
   "language": "python",
   "name": "python37464bitdistributedcomputingconda8b1fedeb11b649b481dffc0658e972f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
