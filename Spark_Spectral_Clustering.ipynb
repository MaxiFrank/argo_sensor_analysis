{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib import linalg\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.csv(\"./digit-recognizer/train.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(arr):\n",
    "    return float(arr[0].dot(arr[1])/\n",
    "                 ((arr[0].dot(arr[0])**0.5) * (arr[1].dot(arr[1])**0.5)))\n",
    "\n",
    "def laplacian_vector(row_id,arr,size,k):\n",
    "    lap_vec = np.zeros(size,dtype=int)\n",
    "    lap_vec[np.array(arr)] = 1\n",
    "    lap_vec[row_id] = -k\n",
    "    return list([int(item) for item in lap_vec])\n",
    "\n",
    "sqlContext.registerFunction('COS_SIM',cosine_similarity,returnType=t.DoubleType())\n",
    "sqlContext.registerFunction(\"LAP_VECTOR\",laplacian_vector,returnType=t.ArrayType(t.IntegerType()))\n",
    "\n",
    "\n",
    "\n",
    "class Spectral_Clustering():\n",
    "    def __init__(self,k=2,k_nearest=7,num_eigenvectors = 10,featureCol='features',predictionCol='predictions'):\n",
    "        self.k = k\n",
    "        self.k_nearest = k_nearest\n",
    "        self.num_eigenvectors = num_eigenvectors\n",
    "        self.featureCol = featureCol\n",
    "        self.predictionCol = predictionCol\n",
    "    def cluster(self, df):\n",
    "        sqlContext = SQLContext(SparkContext.getOrCreate())\n",
    "        n = df.count()\n",
    "        # index rows\n",
    "        df_index = df.select((row_number().over(Window.partitionBy().orderBy(self.featureCol)) - 1).alias('id'),\"*\")\n",
    "        df_features = df_index.select('id',self.featureCol)\n",
    "        sqlContext.registerDataFrameAsTable(df_features,'feature_table')\n",
    "        # k nearest neighbors\n",
    "        knn = sqlContext.sql(f'''\n",
    "                               SELECT\n",
    "                                   *\n",
    "                               FROM\n",
    "                                (SELECT\n",
    "                                    left_id,\n",
    "                                    right_id,\n",
    "                                    RANK() OVER (PARTITION BY left_id ORDER BY norm) AS rank,\n",
    "                                    norm\n",
    "                                FROM \n",
    "                                 (SELECT \n",
    "                                    lq.id as left_id,\n",
    "                                    rq.id as right_id,\n",
    "                                    lq.{self.featureCol} as left_features,\n",
    "                                    rq.{self.featureCol} as right_features,\n",
    "                                    COS_SIM(ARRAY(lq.{self.featureCol},rq.{self.featureCol})) as norm\n",
    "                                 FROM \n",
    "                                    feature_table as LQ\n",
    "                                    JOIN\n",
    "                                    feature_table as RQ\n",
    "                                    on LQ.id != RQ.id)\n",
    "                               )\n",
    "                               WHERE\n",
    "                                   rank <= {self.k_nearest}\n",
    "                             ''')\n",
    "        sqlContext.registerDataFrameAsTable(knn,'knn')\n",
    "        # compute laplacian\n",
    "        laplacian = sqlContext.sql(f\"\"\"\n",
    "                                    SELECT left_id, LAP_VECTOR(left_id,nn,{n},{self.k_nearest}) as lap_vector\n",
    "                                    FROM\n",
    "                                     (SELECT \n",
    "                                        left_id,\n",
    "                                        collect_list(right_id) as nn\n",
    "                                      FROM \n",
    "                                          knn\n",
    "                                      GROUP BY\n",
    "                                      1)\n",
    "                                    ORDER BY\n",
    "                                        1\n",
    "                                    \"\"\")\n",
    "        laplacian_matrix = RowMatrix(laplacian.select('lap_vector').rdd.map(lambda x:list(x[0])))\n",
    "        eigenvectors = laplacian_matrix.computePrincipalComponents(k=self.num_eigenvectors)\n",
    "        \n",
    "        eigenvectors = [(idx,Vectors.dense([float(item) for item in row])) \n",
    "                        for idx, row in enumerate(eigenvectors.toArray().tolist())]\n",
    "        \n",
    "        eigen_df = ss.createDataFrame(eigenvectors,['id',self.featureCol])\n",
    "        model = KMeans(featuresCol=self.featureCol,predictionCol=self.predictionCol,k=self.k).fit(eigen_df)\n",
    "        predictions = model.transform(eigen_df).join(df_index,on='id')\n",
    "        return predictions\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_clust = Spectral_Clustering()\n",
    "assembler = VectorAssembler(outputCol='features',inputCols=df.columns[1:])\n",
    "df_features = assembler.transform(df.limit(1000))\n",
    "spec_clust.cluster(df_features).select('id','predictions','label').show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('distributedcomputing': conda)",
   "language": "python",
   "name": "python37464bitdistributedcomputingconda8b1fedeb11b649b481dffc0658e972f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
